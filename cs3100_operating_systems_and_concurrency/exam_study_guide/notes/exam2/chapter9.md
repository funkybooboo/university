# Chapter 9

Purpose

- Provide a detailed description of various ways of organizing memory hardware.
- Discuss various memory management techniques.
- Introduce paging and segmentation.

## Background (9.1)

### Basic Hardware

- **Main memory** and the registers and cache built into the CPU are the only storage the CPU can access.
    - Instructions include memory addresses, but not disk addresses.

#### Registers

- Built into each CPU core.
- Can be accessed in a single CPU cycle.

#### Main Memory

- Takes many cycles to read data in main memory.
- The CPU normally stalls when this happens, which is not ideal since memory access happens constantly.

#### Cache

- Data storage between memory and registers.
- Hardware automatically controls cache.
- This enables a multithreaded core.

### Protection

- Speed isn't the only concern; we must ensure proper execution of instructions.
- Memory space can be allocated per process.
    - This prevents one process from accessing data for another process.

#### Base and Limit Registers

- These register values identify the range of memory accessible by a process.
    - **Base** holds the smallest possible value.
    - **Limit** specifies the size of the range.
    - The operating system manages these registers in kernel mode.
    - The CPU compares every address generated in user mode with the base and limit register values.

### Address Binding

- A program must be brought into memory to be run.
- The program exists at some memory address location, which is not necessarily the same during execution.
- The addressing in the program instructions are typically symbolic, meaning they are not actual memory addresses since the program doesn't know where it will be located.

#### Three times instructions and data are bound to memory addresses:

1. **Compile Time**:
    - If memory locations are known when a program is compiled, absolute addressing may be used.
    - Must recompile if the location changes.

2. **Load Time**:
    - Binding occurs when a program is loaded into memory.
    - If a starting address changes, then the program must be reloaded.

3. **Execution Time**:
    - Binding happens while the program is executing.
    - Required if the process can move in memory while running.
    - This requires special hardware to handle it and is the most common method.

### Logical vs. Physical Address Space

- **Logical Address** (Virtual Address):
    - Generated by the CPU.

- **Physical Address**:
    - The address seen by the memory unit/controller.
    - The actual address in physical memory.

- **User programs** only ever see the logical address.

#### Memory Management Unit (MMU)

- Hardware device that maps virtual to physical addresses.
    - The base register value is used by the MMU and is called the **Relocation Register**.
    - The user program and CPU work with logical addresses, and the MMU applies the relocation register (by adding the value) to get the actual physical address.

### Dynamic Loading

- Also called **Overlays**.
- A routine, or part of a program, is not loaded into memory until it is needed.
    - This provides more efficient use of memory.
    - Routines are compiled in a relocatable format, meaning they can be loaded and located by the calling routine.
    - If a routine is not in memory when called, it will be loaded and the program's address tables will be updated.
    - No special OS support is required; this is typically implemented by the developer.
    - Note: Modern OSes often do not allow dynamic loading due to memory protection issues.

### Dynamic Linking and Shared Libraries

- **Dynamically Linked Libraries (DLL)**:
    - System libraries linked to user programs during execution.

- **Static Linking**:
    - System libraries are combined by the OS program loader to create a single unified program image.

- **Dynamic Linking**:
    - Linking happens at execution time.
    - A single instance of a DLL can be shared between processes.
    - When a program references a DLL, the program loader loads the DLL into memory (if necessary) and adjusts addresses to the location in memory where the DLL was placed.
    - OS support is required.

### Difference between Dynamic Loading and Dynamic Linking

- **Dynamic Loading**:
    - Made possible at compile time.
    - The address of the loaded routine is known and always the same relative to the main program in memory.

- **Dynamic Linking**:
    - The linked library can be located anywhere.
    - Requires OS support to find and use the address.

## Contiguous Memory Allocation (9.2)

### Memory Organization

- Memory is typically divided into two main sections:
    - **OS (Operating System)**:
        - Can be placed at lower or high address space.
        - Commonly placed in high address space.
    - **User Processes**:
        - The bulk of memory space is allocated to user processes.
        - Each process has its own allocated space.

### Allocating Memory to Processes

- Memory allocation can be very efficient or inefficient.
- This section discusses issues related to **contiguous memory allocation**, where each process is allocated a contiguous block of memory.

#### Memory Protection

- As mentioned earlier, **relocation registers** and **limit registers** help keep the address space of each process separate.
- When a process is loaded for execution, the associated values (base and limit) are loaded into register locations.
- Every address access is checked against these registers to ensure protection for the OS and other processes.

### Memory Allocation Strategy

#### Variable Partition Scheme

- The OS keeps a table to track which parts of memory are available.
- **Hole**: A contiguous section of unused memory.
    - At system start, the user memory space is considered one giant hole.

- Each partition is for a single process.
    - Each process may need a different amount of memory, which is why it's called **variable partition**.
    - Example: A memory allocation scenario with processes (2, 8, 5, etc.), where holes are created after processes finish, and new processes are allocated based on available space.

#### Example of Dynamic Storage Allocation Problem

- When new processes arrive and the holes between memory allocations are too small to fit them, there is a decision on how to handle it.
    - Should a process be moved to combine holes, or should the new process be rejected?
    - Moving processes around can be an expensive operation.

- Common approaches for memory allocation in the variable partition scheme include:
    - **First Fit**: Allocate to the first hole that fits.
    - **Best Fit**: Allocate to the smallest hole that is big enough, leaving the smallest possible hole.
    - **Worst Fit**: Allocate to the largest hole, leaving the largest possible hole for future processes.

#### Results from Simulations

- **Worst Fit** is generally the least efficient.
- **First Fit** and **Best Fit** are quite similar in terms of speed and storage allocation efficiency.

### Memory Fragmentation

- **Fragmentation** refers to memory that is wasted and cannot be used efficiently. It can either be:
    - **External Fragmentation**:
        - The total memory exists, but it's not contiguous (i.e., there isn't a single large enough hole to fit a process).
        - First fit and best fit suffer from external fragmentation.
        - Though there may be total available space, fragmented holes prevent efficient use of memory.
    - **Internal Fragmentation**:
        - Allocated memory may be larger than needed for a process.
        - The space left over in an allocation is wasted.
        - Sometimes, it's more cost-effective to leave small holes rather than track them separately.

- Both **external** and **internal fragmentation** can occur at the same time.

#### The 50-Percent Rule

- An analysis of the **first fit** allocation method shows that for every N blocks allocated, approximately 0.5N blocks will be lost to fragmentation.
    - This means that up to 1/3 of memory may end up being unusable due to fragmentation, which is significant.

### Compaction

- **Compaction** is a process that aims to shuffle memory to place all allocated partitions in a single contiguous block.
    - This is only possible if **address binding** is dynamic and occurs at runtime.
    - This entails moving all data and changing the base register values.
    - Compaction can be **expensive** in terms of system resources, as it requires relocating large amounts of memory and updating address tables.


## Paging (9.3)

### Introduction to Paging

- Previous memory allocation approaches required contiguous blocks of memory.
- **Paging** allows a process's physical memory to be non-contiguous.
- This method is commonly used by most operating systems (OS).

### Basics of Paging

- **Physical Memory**: Divided into fixed-size blocks called **frames**. Frame size is typically a power of 2 (usually between 4KB to 1GB).
- **Logical Memory**: Also divided into fixed-size blocks, called **pages**, with the same size as frames.
    - **Frame**: Physical memory block that holds data.
    - **Page**: Logical memory block that holds program instructions/data.

- **Operating System (OS)**:
    - Tracks empty frames in memory.
    - Allocates the required number of frames to a process based on its pages.
    - It is even possible to allocate more frames than physically available (a concept discussed further below).

### CPU Addressing in Paging

- Each logical address is split into two parts:
    1. **Page Number (p)**: Identifies the page of data/instructions needed.
    2. **Page Offset (d)**: Specifies the location within the page where the data/instruction is located.

### Page Table

- Every process maintains a **page table** to map its logical addresses to physical ones.
    - The page number from the logical address is used to find a corresponding **frame number**.
    - The offset remains unchanged during translation from page to frame.

- **Example**:
    - Logical memory may contain pages like page 0, page 1, page 2, etc.
    - A page table maps these pages to frames in physical memory (e.g., page 0 → frame 3, page 1 → frame 4, etc.).

- The OS treats pages as contiguous, even though they may be scattered in memory.

### Fragmentation in Paging

- **Internal Fragmentation**: The last page allocated may not be fully used, leading to wasted space. This is the "remainder" page.
    - Paging still suffers from **internal fragmentation**, but it avoids **external fragmentation**.
    - Average internal fragmentation is about half a page per process; best case is zero, and the worst case is one byte (or a whole frame).

- **No External Fragmentation**: Unlike contiguous memory allocation, where holes might prevent efficient memory usage, paging ensures that all memory is utilized, even if it’s non-contiguous.

### Page Sizing

- Page size generally ranges between **4KB to 1GB**, with **4KB to 16KB** being the most typical.
- For example, the Mac M1 uses a page size of **16384 Bytes (16KB)**, requiring 14 bits to address a page (2^14 = 16,384).

- Larger pages reduce overhead but increase internal fragmentation, as more memory might be allocated than needed.

### Logical Addressing

- **Logical Addressing**: Involves breaking the logical address into a **page number** and an **offset**.
    - For a logical address space of \( 2^m \) bits and page size of \( 2^n \) bytes:
        - **Page Offset** requires **n** bits.
        - **Page Number** requires **m - n** bits.

- **Example**:
    - If a process size is **16 bytes**, it needs **4 bits** to address it (since \( 2^4 = 16 \)).
    - With a page size of **4 bytes**, **2 bits** are required for the page offset.
    - This process needs **4 pages**, resulting in **no internal fragmentation**.
    - If physical memory is **32 bytes**, or 8 pages, the logical to physical address mapping will look like:
        - Logical address `0` maps to physical address `20`.
        - Logical address `3` maps to physical address `23`, and so on.

### Problem Example

For a process size of **72,766 bytes** and a page size of **2,048 bytes**:
- **Pages Needed**:
    - \( 72,766 \div 2048 = 35.53 \), so **36 pages** are required.
- **Internal Fragmentation**:
    - \( 72,766 \mod 2048 = 1,086 \) bytes.
    - The remaining **962 bytes** are wasted, as they do not fit into a full page.
- **Logical Address Size**:
    - \( \log_2(72,766) = 16.15 \), so **17 bits** are required for the logical address.
    - **Offset size**: For **2,048-byte** pages, **11 bits** are needed.
    - **Page Count**: \( 17 - 11 = 6 \) bits needed to address the pages.

### Allocation Process

- When a process arrives, the OS calculates the number of pages it requires and allocates the necessary frames.
- The OS keeps a **Frame Table**, which tracks the allocation of each frame:
    - Each frame has an entry that specifies whether it is free or allocated and if allocated, which page it holds.

### Page Table Implementation

- The page table is stored in memory, but a **Page Table Base Register (PTBR)** indicates the starting address of the page table.
- Accessing the page table requires two memory accesses:
    1. First, to find the frame number in the page table.
    2. Second, to fetch the data from the corresponding physical memory address.

#### Translation Look-Aside Buffer (TLB)

- **TLB** is a special cache designed to speed up memory access. It holds a small number of recent page table entries (typically 64-1024 entries).
    - **TLB Hit**: The frame number is found in the TLB, resulting in fast access to the data.
    - **TLB Miss**: If the page isn't in the TLB, the page table is accessed as usual, and the TLB is updated.

#### Address Space Identifiers (ASIDs)

- **ASIDs** are stored with each TLB entry to associate the entry with a specific process, ensuring memory protection across multiple processes.

### Memory Protection

- Page tables can include additional bits for memory protection:
    - **Read/Write Bit**: Determines if a page can be modified. If a write operation is attempted on a read-only page, an interrupt is triggered.
    - **Execute Bit**: Specifies if a page is executable, helping prevent security vulnerabilities (e.g., preventing arbitrary code execution).
    - **Valid/Invalid Bit**: Indicates if a page entry is valid or not, helping to ensure that the program doesn't access invalid memory locations.

### Shared Pages

- If multiple processes use the same code (like a library), the code can be shared among them. This is known as **reentrant code**.
    - Shared code must be read-only to avoid modification by any of the processes.
    - This reduces overall memory usage as multiple processes can use the same physical memory for common code.

### Conclusion

Paging is an essential memory management technique that allows efficient and flexible allocation of memory. It avoids external fragmentation and simplifies memory access, although it does introduce internal fragmentation. Through techniques like TLBs and page tables, paging offers fast memory access and robust memory protection.


## Structure of the Page Table (9.4)

### Introduction to Page Table Structures

- So far, we've considered basic page table implementations, but real-world implementations are more sophisticated.
- These enhanced structures are necessary due to the potential size of the page table. When the page table grows larger than a single page, allocating contiguous memory can become problematic.
- The book discusses three common page table structures:
  1. **Hierarchical Page Table** (which we'll focus on)
  2. **Hashed Page Table**
  3. **Inverted Page Table**

### Hierarchical Page Table Structure

- This structure uses a **multi-level page table** approach, where the page table references other page tables, rather than directly referencing physical frames.
- The logical address is divided into multiple parts, each pointing to different levels of page tables.

#### Example:

Consider a **32-bit logical address** with a **page offset of 12 bits**:
- The address is divided into:
  1. **P1** (the outer page table index, 10 bits)
  2. **P2** (the inner page table index, 10 bits)
  3. **d** (the page offset, 12 bits)
  
- In this structure:
  - **Outer page table**: The first part of the page number (P1) indexes into the outer page table, which contains pointers to inner page tables.
  - **Inner page table**: The second part (P2) indexes into the inner page table, which points to the actual frame in physical memory.

The page table can have multiple levels, with each additional level further splitting the logical address. This hierarchical structure reduces the need for large, contiguous memory allocations for the page table.

### Example of the Multi-Level Structure:

- **Logical Address (P1 P2 d)**:
  - **Outer page table**: Points to the page tables.
  - **Inner page table**: Points to the frames in physical memory.
  - **d**: The offset within the frame.
  
- The multi-level page table helps manage the size of the page table efficiently.

## Swapping (9.5)

### What is Swapping?

- **Swapping** allows a process that is currently idle to be temporarily removed from memory, freeing up space for other processes.
- This helps to create the illusion that the total logical address space of processes is larger than the available physical memory.

### Backing Store

- The **backing store** is a location (usually on the hard drive or secondary storage) where processes are swapped out of memory and stored temporarily when they are not actively in use.
- The OS must track processes that are swapped out, so they can be restored to memory when needed.

### Approaches to Swapping

1. **Standard Swapping**:
   - The entire process is removed from memory and placed in the backing store.
   - All data structures associated with the process (including code, data, and stack) are swapped out.
   - The OS keeps track of swapped-out processes to restore them when needed.
   - Swapping incurs significant overhead due to the time it takes to transfer data between memory and storage.
   - **Candidate processes** for swapping are typically idle or mostly idle, as they consume memory but aren't doing useful work.

2. **Swapping with Paging**:
   - Instead of swapping out entire processes, **individual pages** can be swapped in and out of memory. 
   - This is often referred to as **paging**, and it greatly reduces the overhead of transferring data compared to swapping entire processes.
   - Pages that are not in active use can be swapped out while still allowing the process to execute.

### Swapping on Mobile Devices

- Mobile devices typically don't support full swapping due to the following reasons:
  1. **Limited flash memory**: Flash storage is smaller compared to hard disks.
  2. **Limited write endurance**: Flash memory has a finite number of write cycles before becoming unreliable.
  3. **Lower throughput**: The speed of data transfer between mobile flash storage and the CPU is slower than on desktop systems.

- **Solutions for Mobile Devices**:
  1. **iOS**:
     - iOS limits swapping by asking apps to voluntarily relinquish memory.
     - **Read-only data** may be discarded and reloaded when needed.
     - **Modified data** (e.g., the stack) is never swapped out.
     - Applications are terminated when memory is low, ensuring that new processes can be loaded.
  
  2. **Android**:
     - Android manages memory by terminating apps when more memory is needed.
     - It saves the application's state to flash memory, enabling a quick restart when the app is resumed.

### Conclusion

Swapping helps manage memory by allowing processes to be temporarily removed from physical memory. While this can be done by swapping entire processes or swapping individual pages, mobile devices use specialized methods to manage memory due to their hardware limitations. The hierarchical page table structure offers an efficient way to manage memory by breaking down the page table into multiple levels, which helps reduce the memory overhead involved in managing large address spaces.
