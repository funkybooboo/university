{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/funkybooboo/CS5060_Program4/blob/main/hw4_Reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 4: Reinforcement Learning\n",
        "## Contributors:\n",
        "- Brighton Ellis\n",
        "- Ann Marie Humble\n",
        "- Nate Stott\n",
        "- Maddie Patch\n",
        "___"
      ],
      "metadata": {
        "id": "xWMhwXOCaKZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install stabel baselines3 and Gym"
      ],
      "metadata": {
        "id": "DVJ5gw9HZRpC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2zXgC4yXw-s",
        "outputId": "8d74ca34-634d-4ab0-c11c-c9b9f4a111e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.5.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3 gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n"
      ],
      "metadata": {
        "id": "gY0m_l1iYqsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. setup the cartPole env and baseline training"
      ],
      "metadata": {
        "id": "1qaEjQO7Y9dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Train the baseline model\n",
        "baseline_model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "baseline_model.learn(total_timesteps=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz44bx0sYsrI",
        "outputId": "b8d4dad1-a636-4ee5-b961-7b2c92c21867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 21.3     |\n",
            "|    ep_rew_mean     | 21.3     |\n",
            "| time/              |          |\n",
            "|    fps             | 296      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 6        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7f4347fc6410>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Record average baseline reward*"
      ],
      "metadata": {
        "id": "Y6ivzO_SceZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Evaluate baseline agent performance"
      ],
      "metadata": {
        "id": "lluZA_nJZCt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_rewards = []\n",
        "episodes = 10\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action, _ = baseline_model.predict(state, deterministic=True)\n",
        "        state, reward, done, _, _ = env.step(action)  # Ensure you're unpacking correctly\n",
        "        episode_reward += reward\n",
        "\n",
        "    baseline_rewards.append(episode_reward)\n",
        "\n",
        "avg_baseline_reward = sum(baseline_rewards) / episodes\n",
        "print(f\"Average reward with baseline model: {avg_baseline_reward}\")\n"
      ],
      "metadata": {
        "id": "S74vLomDYvuw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5eeea5-b708-4ae1-83f7-095aaa7ef367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward with baseline model: 388.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Modify the reward function"
      ],
      "metadata": {
        "id": "1-nNZMxiZItm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gymnasium import RewardWrapper\n",
        "\n",
        "class CustomCartPoleReward(RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(CustomCartPoleReward, self).__init__(env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        # Access the CartPole state and penalize based on the pole's tilt\n",
        "        x, x_dot, theta, theta_dot = self.env.state\n",
        "        new_reward = reward - np.abs(theta)  # Penalize for angle from upright\n",
        "        return new_reward\n",
        "\n",
        "# Initialize the custom environment\n",
        "custom_env = CustomCartPoleReward(gym.make(\"CartPole-v1\"))\n"
      ],
      "metadata": {
        "id": "sJHedyfgZeHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. train the agent with modified reward function"
      ],
      "metadata": {
        "id": "EEv-tRzbZfGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model = PPO(\"MlpPolicy\", custom_env, verbose=1)\n",
        "custom_model.learn(total_timesteps=10000)\n"
      ],
      "metadata": {
        "id": "AfcLHmmoZkdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a33bf0-189c-4b2d-99c3-1cfd2cfff2e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.state to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.state` for environment variables or `env.get_wrapper_attr('state')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 23       |\n",
            "|    ep_rew_mean     | 21.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 556      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 26          |\n",
            "|    ep_rew_mean          | 23.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 410         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008403098 |\n",
            "|    clip_fraction        | 0.0928      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.687      |\n",
            "|    explained_variance   | 0.000179    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.02        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0139     |\n",
            "|    value_loss           | 42.3        |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 34        |\n",
            "|    ep_rew_mean          | 31.2      |\n",
            "| time/                   |           |\n",
            "|    fps                  | 398       |\n",
            "|    iterations           | 3         |\n",
            "|    time_elapsed         | 15        |\n",
            "|    total_timesteps      | 6144      |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0098195 |\n",
            "|    clip_fraction        | 0.0645    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.666    |\n",
            "|    explained_variance   | 0.101     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 9.73      |\n",
            "|    n_updates            | 20        |\n",
            "|    policy_gradient_loss | -0.0181   |\n",
            "|    value_loss           | 29        |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 46          |\n",
            "|    ep_rew_mean          | 42.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 382         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 21          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010336495 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.633      |\n",
            "|    explained_variance   | 0.225       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 17.8        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0219     |\n",
            "|    value_loss           | 47.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 60.1        |\n",
            "|    ep_rew_mean          | 55.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 386         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 26          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008151073 |\n",
            "|    clip_fraction        | 0.0655      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.615      |\n",
            "|    explained_variance   | 0.261       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 17.6        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0146     |\n",
            "|    value_loss           | 55.8        |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7f42cfb17130>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. evaluate the modified agent's performance"
      ],
      "metadata": {
        "id": "0XQDabMBZmtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_rewards = []\n",
        "episodes = 10\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state, _ = custom_env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action, _ = custom_model.predict(state, deterministic=True)\n",
        "        state, reward, done, _, _ = custom_env.step(action)  # Assuming step returns 4 values\n",
        "        episode_reward += reward\n",
        "\n",
        "    custom_rewards.append(episode_reward)\n",
        "\n",
        "avg_custom_reward = sum(custom_rewards) / episodes\n",
        "print(f\"Average reward with custom reward function: {avg_custom_reward}\")\n"
      ],
      "metadata": {
        "id": "3s_KQQD4ZnKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56484f9a-72df-403b-fd63-42541c4bad72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward with custom reward function: 415.7299787929701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can compare `avg_custom_reward` to `avg_baseline_reward` and record observations on how agent's performance was affected"
      ],
      "metadata": {
        "id": "nG7fVgi_Zu6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. modify the model architecture"
      ],
      "metadata": {
        "id": "OwXNiLMUZ-3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "import torch as th\n",
        "from torch import nn\n",
        "\n",
        "# Custom network architecture\n",
        "class CustomMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomMLP, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(4, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Define the model with custom architecture\n",
        "complex_model = PPO(\"MlpPolicy\", env, policy_kwargs=dict(activation_fn=th.nn.ReLU, net_arch=[128, 128]), verbose=1)\n",
        "complex_model.learn(total_timesteps=10000)\n"
      ],
      "metadata": {
        "id": "AH5jXqxyaCdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fdf1f07-1ae3-47c9-e4c7-6f7f791ae448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 20.9     |\n",
            "|    ep_rew_mean     | 20.9     |\n",
            "| time/              |          |\n",
            "|    fps             | 545      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 26.1        |\n",
            "|    ep_rew_mean          | 26.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 447         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010955214 |\n",
            "|    clip_fraction        | 0.0968      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.686      |\n",
            "|    explained_variance   | -0.0147     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.81        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0161     |\n",
            "|    value_loss           | 44.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 35.8        |\n",
            "|    ep_rew_mean          | 35.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 390         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 15          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009241875 |\n",
            "|    clip_fraction        | 0.0409      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.666      |\n",
            "|    explained_variance   | 0.0401      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 14          |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0145     |\n",
            "|    value_loss           | 36.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 49.1        |\n",
            "|    ep_rew_mean          | 49.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 387         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 21          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009751111 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.631      |\n",
            "|    explained_variance   | 0.265       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 20.9        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0223     |\n",
            "|    value_loss           | 47.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 62.8        |\n",
            "|    ep_rew_mean          | 62.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 379         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 26          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008121518 |\n",
            "|    clip_fraction        | 0.0692      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.61       |\n",
            "|    explained_variance   | 0.225       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 23          |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0164     |\n",
            "|    value_loss           | 55.9        |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7f4347fc64a0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Evaluate the complex model's performance"
      ],
      "metadata": {
        "id": "94HAEhNyaFCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "complex_rewards = []\n",
        "episodes = 10\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action, _ = complex_model.predict(state, deterministic=True)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "\n",
        "    complex_rewards.append(episode_reward)\n",
        "\n",
        "avg_complex_reward = sum(complex_rewards) / episodes\n",
        "print(f\"Average reward with complex model architecture: {avg_complex_reward}\")\n"
      ],
      "metadata": {
        "id": "DwJIvYQcaH_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac841f2a-fbfa-4ae9-e774-5b7823543577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward with complex model architecture: 509.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW4: Reinforcement Learning\n",
        "\n",
        "Due: Fri Nov 15, 2024 11:59pm\n",
        "\n",
        "30 Possible Points\n",
        "\n",
        "Group:\n",
        "\n",
        "\t- Ann Marie Humble\n",
        "\t- Brighton Roghelio Ellis\n",
        "\t- Madison Eve Patch\n",
        "\t- Nathaniel Dana Stott\n",
        "\n",
        "## Summary of Observations\n",
        "\n",
        "Compare the results from the baseline model, the modified reward function model, and the complex architecture model. Document your observations:\n",
        "\n",
        "- Baseline Model Average Reward: `avg_baseline_reward`\n",
        "  - Average over seven executions: 267.3\n",
        "- Custom Reward Model Average Reward: `avg_custom_reward`\n",
        "  - Average over seven executions: 508.6\n",
        "- Complex Model Architecture Average Reward: `avg_complex_reward`\n",
        "  - Average over seven executions: 439.1\n",
        "\n",
        "__Based on the average rewards observed, how did each modification affect the agent’s performance?__\n",
        "\n",
        "From the average rewards observed, we can see that the baseline model did the worst, the complex architecture model did better (with about two times the average reward of the baseline model), and the custom reward model did the best (with about 2.5 times the average reward of the baseline model).\n",
        "\n",
        "__Why did baseline behave like that?__\n",
        "\n",
        "With each of these models, our aim is to keep a (simulated) pole balanced atop a cart for as long as possible. The action space has only two options: push the cart to the left, or push the cart to the right. The observation space can see the cart position, cart velocity, pole angle, and pole angular velocity.\n",
        "\n",
        "Our baseline model accumulates rewards based on how long the pole stays balanced on the cart. The policy must pick paths for the cart that maximize rewards, but since the rewards are vague, the simulation must explore a lot to find which paths work best. The baseline model also updates its policy using the PPO objective, which ensures that only small changes are made to the policy every cycle. This means that the model will take longer to learn, but will hopefully move in a more stable upward direction. PPO also won’t learn between runs, so if you re-run it, you’re starting over from the beginning. All of these factors play into the reasons why our baseline model performed the worst.\n",
        "\n",
        "__Why did complex architecture behave like that?__\n",
        "\n",
        "The complex architecture model uses a custom neural network architecture instead of the default architecture used in the baseline model. The complex architecture model has fewer neurons than the baseline model, but deeper layers. This can improve the model’s performance on more complex tasks or environments, but could also lead to slower training and overfitting if the task is too simple. In our case, the complex architecture had better results than our baseline model, so we know that the task wasn’t too simple. However, the complex architecture model did worse than the custom reward model. This makes sense, because the complex model increased its complexity without fine-tuning its rewards.\n",
        "\n",
        "__Why did custom reward behave like that?__\n",
        "\n",
        "The custom reward model specifically rewards and penalizes based on the pole’s tilt. Aside from this, the model has the same environment as the baseline model. This is more specific than the baseline reward model, because it tells the model what to do specifically to keep the pole in the air. Even though this model isn’t beefed up (like the complex architecture model is), it is more targeted. Therefore, the model is able to learn faster how to keep the pole in the air, and the rewards are higher.\n",
        "\n",
        "__Discuss any trends or behaviors noticed in each model setup.__\n",
        "\n",
        "All model setups we tested had very similar rollout values, with the complex model having the largest. The values for total_timesteps, time_elapsed, and iterations are also the same. The only value that truly differs between models is fps, which is smaller for the custom rewards model, and largest for the complex model.\n",
        "\n",
        "### Baseline\n",
        "| Metric              | Value   |\n",
        "|---------------------|---------|\n",
        "| **ep_len_mean**     | 20.9    |\n",
        "| **ep_rew_mean**     | 20.9    |\n",
        "| **fps**             | 720     |\n",
        "| **iterations**      | 1       |\n",
        "| **time_elapsed**    | 2       |\n",
        "| **total_timesteps** | 2048    |\n",
        "\n",
        "---\n",
        "\n",
        "### Custom\n",
        "| Metric              | Value   |\n",
        "|---------------------|---------|\n",
        "| **ep_len_mean**     | 21.5    |\n",
        "| **ep_rew_mean**     | 19.7    |\n",
        "| **fps**             | 698     |\n",
        "| **iterations**      | 1       |\n",
        "| **time_elapsed**    | 2       |\n",
        "| **total_timesteps** | 2048    |\n",
        "\n",
        "---\n",
        "\n",
        "### Complex\n",
        "| Metric              | Value   |\n",
        "|---------------------|---------|\n",
        "| **ep_len_mean**     | 21.9    |\n",
        "| **ep_rew_mean**     | 21.9    |\n",
        "| **fps**             | 729     |\n",
        "| **iterations**      | 1       |\n",
        "| **time_elapsed**    | 2       |\n",
        "| **total_timesteps** | 2048    |\n",
        "\n"
      ],
      "metadata": {
        "id": "L-kmv3jHawdE"
      }
    }
  ]
}